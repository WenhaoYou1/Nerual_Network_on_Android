\documentclass[UTF8]{article}
\usepackage{cite}
\usepackage[unicode,pdftex]{hyperref}
\usepackage{enumerate}
% \usepackage{geometry}
\usepackage{setspace}
\usepackage{pslatex} 
\usepackage{fancyhdr}
\usepackage{float}
\usepackage{amsmath}
\usepackage{titling}
\usepackage{indentfirst}
\usepackage{graphicx}
\usepackage{wrapfig}
\usepackage{amsmath}
\usepackage{xstring}
\usepackage{tikz}
\usetikzlibrary{fit, calc}
\pagestyle{empty} 
\usepackage{array}


% \geometry{left=2.5cm, right=2.5cm, top=1.4cm, bottom=2.4cm}
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1in, margin=1in]{geometry}
% \usepackage[margin=1in]{geometry}
\title{xxxxxxxxxxxxxxxxxxxxxxxx xxxxxxxxxxxxxxxxxxxxxxxx}
\author{Wenhao You(wyou1@ualberta.ca) Leo Chang(chewei@ualberta.ca)}
\date{}


\newcommand{\reffig}[1]{Fig. \ref{#1}}
\newcommand{\refsec}[1]{Section \ref{#1}}
% \newcommand{\refeq}[1]{Eq. \ref{#1}}
\newcommand{\reftab}[1]{Table \ref{#1}}


\fancyhead{}
\lhead{\scriptsize Chenqiu Zhao}
\rhead{\scriptsize research Proposal}

\renewcommand{\headrulewidth}{0pt}
\renewcommand{\normalsize}{\fontsize{12pt}{\baselineskip}\selectfont}


\newcommand{\chronoperiode}[7]{
	\pgfmathsetmacro{\first}{(#2 - 2018)*12 + #3 - .9} % beginig of the peropd
	\pgfmathsetmacro{\last}{(#4 - 2018)*12 + #5 - 1.1} % end of the period
	\pgfmathsetmacro{\middle}{(\first+\last)/2} % position of the country name
	\fill[#7] (\first,#6-1) rectangle (\last,#6) (\middle,#6-.5) node[white, font=\sf]{#1};
}

\definecolor{level1}{RGB}{200,10,20}
\definecolor{brightube}{rgb}{0.82, 0.62, 0.91}
\definecolor{fuchsia}{rgb}{1.0, 0.0, 1.0}
\definecolor{heliotrope}{rgb}{0.87, 0.45, 1.0}



\begin{document}
	\maketitle
	%\vspace{-90pt}
	
	
	
	
	
	
	\section*{Abstract}
	
	xxxxxxxxxxxxxxxxxxxxxxxx
	
	xxxxxxxxxxxxxxxxxxxxxxxx
	
	xxxxxxxxxxxxxxxxxxxxxxxx
	
	xxxxxxxxxxxxxxxxxxxxxxxx
	
	xxxxxxxxxxxxxxxxxxxxxxxx
	
	xxxxxxxxxxxxxxxxxxxxxxxx
	
	xxxxxxxxxxxxxxxxxxxxxxxx
	
	xxxxxxxxxxxxxxxxxxxxxxxx
	
	xxxxxxxxxxxxxxxxxxxxxxxx
	
	\section*{Introduction}
	
	%\begin{wrapfigure}{l}{0.5\textwidth}
	%  \vspace{-15pt}    % 对应高度1
	%  \includegraphics[width=0.5\textwidth]{figure/fig1.pdf}\\
	%   \label{fig1}
	%  \vspace{-15pt}    % 对应高度2
	% \caption{Deep distribution Learning for Background Subtraction.}
	% \label{fig1}
	%  \vspace{-15pt}    % 对应高度3
	% \end{wrapfigure}
Neural networks have been implemented on many applications and performed a great results. However, these neural networks are mostly implemented on a high-specification hardware such as computers since there are many parameters in different neural networks. The more parameters in the neural networks, the more storages and internet transmition rates are occupied. In this research, we try to look for a method that can make the neural networks run on a limited hardware devices, such as mobile devices. Although mobile devices have more enhanced hardware nowadays, but because of its limited size, the efficacy is still not as powerful as the computers. There are several benefits of running the neural networks on limited hardware devices, specifically, mobile devices. The most important part is the privacy and the data security. Processing data on-device ensures that personal information does not need to upload or transmit to the cloud servers, which enhanced privacy. 

Running neural networks on mobile devices can bring lots of advantages. Mobile devices, such as smartphones and tablets, play a significant role in our daily lives. If we are able to implement the neural network models directly on the mobile devices, it can reduce our dependence on the internet connection to some extent. Moreover, for some applications which need real-time feedback, such as object recognition and image segmentation, neural networks on mobile devices can also make the process faster and improve the user experience. The most important part of modern life is that we can keep the data processing and inference on our own interal system of the mobile devices rather than sending it to a remote server, which can enhance privacy and security to a great extent.

The new method called Frequency regularization (FR)~\cite{zhao2023frequency} is based on the frequency domain to compress the neural network. In this way, it can be divided into two steps: dynamic tail-truncation and inverse discrete cosine transform (IDCT). The main idea of this method is that some of the neural netwroks parameters may be redundant in their representation. The algorithm focus on using the frequency domain transform for network regularization to restrict information redundancy during the training process and introduce FR as shown in Fig.~\ref{idct}. Therefore, the redundant part can be removed without any side effect on the performance of the network.

\begin{wrapfigure}{l}{0.5\textwidth}
	\vspace{-15pt}    % 对应高度1
	\includegraphics[width=0.5\textwidth]{figure/idct.png}\\
	\vspace{-15pt}    % 对应高度2
	\caption{Illustration of the proposed frequency regularization. The tail elements of tensors in the frequency domain are truncated first. then input into the inverse of the discrete cosine transform to reconstruct the spatial tensor for learning features~\cite{zhao2023frequency}.}
	\label{idct}
	\vspace{-15pt}    % 对应高度3
\end{wrapfigure}


In order to implement the neural networks on mobile devices, we have to think about the limitations of hardware and its characteristics. Many popular deep learning libraries, such as PyTorch and TensorFlow, are not complete and perfect to implement directly on the mobile devices. Thus, we need to edit some implementation in order to optimize specifically for the mobile devices such as TensorFlow Lite. By using the method, we can convert our standard neural network models into a lite format that can fit the mobile devices hardware better. Moreover, beyond this idea, we can also consider improving the hardware parts such as creating special neural network processors for the mobile. In this paper, we only explore the method in software instead of hardware.

Our paper makes discussion explores a simple, efficient, and creative method to transfer the large neural network into mobile devices, which is based on Frequency regularization. Our purpose is not only to accomplish this implementation but also to make sure the performance which should be the same as running on high-specification hardware such as computers. To sum up, we hope to provide a fast and secure neural network application experience for anyone who uses mobile devices.


% \begin{figure}[!h]
	%     \centering
	%     \includegraphics[width=0.5\textwidth]{../imgs/distribution.pdf}
	%     \caption{Segmenting moving objects in freely moving camera.}
	%     \label{fig_intro}
	% \end{figure}

\section*{Brief Summary of Existing Work}

In this section, we cover the methods about implementing neural network on the mobile over three sections. In Section~\ref{upgrade_hardware}, upgrading hardware on mobile devices and which part are inrtoducted. At the algorithm level, there is an algorithm called NestDNN~\cite{fang2018nestdnn} based on the dynamics or runtime resources to enable resource-aware multi-tenant on-device deep learning for mobile vision systems as shown in Section~\ref{nestdnn}. Moreover, there is an algorithm called “one-shot whole network compression”~\cite{kim2016compression} to compress convolutional neural networks (CNNs) for mobile deployment as shown in Section~\ref{oneshot}.

\subsection{Upgrade Hardware}\label{upgrade_hardware}
Hardware accelerators such as the digital signal processors offer solutions to overcome these challenges. Many manufacturers have already produced the better hardware. For example, Apple presented the Metal Performance Shaders with support of CNNs accelerated by GPU. This is a solution built on top of the Metal API and allows custom perations. Since there is a experiment~\cite{akusok2019metal} which confirmed the feasibility of Big Data analysis on modern mobile devices. Moreover, the hardwares on Android devices are upgraded by four main companies, such as Qualcomm , HiSilicon, MediaTek, and Samsung~\cite{Andrey2019Aibenchmark}, which can be used for neural networks as well.

\subsection{Algorithm NestDNN}\label{nestdnn}
NestDNN is a framework that takes the dynamics of runtime resources into account to enable resourceaware multi-tenant on-device deep learning for mobile vision systems. NestDNN enables each deep learning model to offer flexible resource-accuracy trade-offs~\cite{fang2018nestdnn}. Fig.~\ref{nestdnn_arch} illustrates the architecture of NestDNN detailedly, which is split into an offline stage and an online stage. The offline stage consists of three phases: model pruning, model recovery, and model profiling~\cite{fang2018nestdnn}. In Table~\ref{table_nestdnn}, it illustrates all the terminologies involved in NestDNN.

	\begin{figure}[htbp]
	\centering
	\includegraphics[width=1\linewidth]{figure/nestdnn_architecture.png}
	\caption{NestDNN architecture~\cite{fang2018nestdnn}.}
	\label{nestdnn_arch}
\end{figure}

\begin{table}
	\label{table_nestdnn}
	\centering
	\begin{tabular}{|c|>{\centering\arraybackslash}p{8cm}|}
		\hline
		\textbf{Terminology}&\textbf{Explanation}\\
		\hline
		\textbf{Vanilla Model}&Off-the-shelf deep learning model (e.g., ResNet) trained on a given dataset (e.g., ImageNet). \\
		\hline
		\textbf{Pruned Model}&Intermediate result obtained in model pruning stage.\\
		\hline
		\textbf{Seed Model}&The smallest pruned model generated in model pruning which meets the minimum accuracy goal set by the user. It is also the starting point of model recovery stage.\\
		\hline
		\textbf{Descendant Model}&A model grown upon the seed model in model recovery stage. It has a unique resource-accuracy trade-off.\\
		\hline
		\textbf{Multi-Capacity Model}&The final descendant model that has the capacities of
		all the previously generated descendant models.\\
		\hline
	\end{tabular}
	\caption{Terminologies involved in NestDNN~\cite{fang2018nestdnn}.}
\end{table}


\subsection{Algorithm One-shot Whole Network Compression}\label{oneshot}
"One-shot Whole Network Compression" is a simple way to compress CNNs to make them more suitable for fast and low-power applications on the mobile. This algorithm can be divided into three steps: rank selection, tensor decomposition, and fine-tuning. Each step can be implemented easily by using public tools such as VBMF for rank selection, Tucker decomposition for tensor decomposition, and Caffe for fine-tuning~\cite{kim2016compression}.

\section*{What you Plan To Do}
There are five sub-topics for implementing Frequency Regularization on the Android devices. The first one "Implement Frequency Regularization" have done in Basu's lab in University of Alberta (2023). The remaining topics will be finished before the lecture ends.


\begin{itemize}
	\item Implement Frequency Regularization: Implement the Frequency Regularization in order to compress the redundant information on nerual network. Make sure that the accuracy and velocity is acceptable for the nerual network.
	
	\item Configure Emulator: In this paper, we choose Android Studio as our emulator. Install it successfully on our own local machines.
	
	\item Implement PyTorch Library: Implement PyTorch Android libaray to the emulator. Collect and analyze the error information if we get.
	
	\item Adjust Dependencies: Potential errors may probably caused by Android PyTorch Library. Since the library on android lacks of improvement not as much as laptop.
	
	\item Adjust Frequency Regularization: We can also try to change something on the algorithm to let the new one fitting the Android system.
	
\end{itemize}

\section*{How You Plan to Implement Your Ideas}

In this research, we are only going to put the compressed neural network to the mobile devices and make it run on the emulator. 



\section*{Timeline for Deep Distribution}

xxxxxxxxxxxxxxxxxxxxxxxx

xxxxxxxxxxxxxxxxxxxxxxxx

xxxxxxxxxxxxxxxxxxxxxxxx

\section*{Short Description of 5 LABS}

Deploy the Frequency Regularization algorithm with the UNet (neural network) on the computer and adjust some parameters for the emulator. 

Attempt to implement PyTorch on the emulator and summarize the error messages. 

Analyze the error messages and try to do the debug process. 

XXX

Test all functions and neural networks to check the effectiveness and accuracy on the emulator after importing the compressed neural network. We compare the result between the computer and mobile devices. 



\small
\bibliographystyle{unsrt}
% \bibliographystyle{plain}
\bibliography{ref.bib}  




\end{document}
